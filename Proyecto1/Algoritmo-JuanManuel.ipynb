{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Importación de librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerías para manejo de datos\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 25) # Número máximo de columnas a mostrar\n",
    "pd.set_option('display.max_rows', 50) # Numero máximo de filas a mostar\n",
    "import numpy as np\n",
    "np.random.seed(3301)\n",
    "import pandas as pd\n",
    "# Para preparar los datos\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Para crear el arbol de decisión \n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "# Para usar KNN como clasificador\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# Para realizar la separación del conjunto de aprendizaje en entrenamiento y test.\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Para evaluar el modelo\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_score, recall_score, f1_score, accuracy_score, ConfusionMatrixDisplay\n",
    "# Para búsqueda de hiperparámetros\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Para la validación cruzada\n",
    "from sklearn.model_selection import KFold \n",
    "#Librerías para la visualización\n",
    "import matplotlib.pyplot as plt\n",
    "# Seaborn\n",
    "import seaborn as sns \n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Limpieza y Perfilamiento de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Importación de librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librería Natural Language Toolkit, usada para trabajar con textos.\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#El lemmatizer de NLTK NO funciona en español, por lo que se usará el de Stanza.\n",
    "\n",
    "import stanza\n",
    "stanza.download('es')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = stanza.Pipeline(lang='es', processors='tokenize,mwt,pos,lemma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalación de librerias\n",
    "import sys\n",
    "import re, string, unicodedata\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Lectura de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uso de la libreria pandas para la lectura de archivos\n",
    "data=pd.read_csv('train_reviews.csv', sep=',', encoding = 'utf-8')\n",
    "# Asignación a una nueva variable de los datos leidos\n",
    "data_train=data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Entendimiento de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats as st\n",
    "\n",
    "textos = data_train.copy()\n",
    "textos['Conteo'] = [len(x) for x in textos['Review']]\n",
    "\n",
    "#Por ahora: La moda no da información relevante\n",
    "\"\"\"def moda(textos):\n",
    "    for i in textos['Review']: \n",
    "        dict = {}\n",
    "        for x in i.split(' '): \n",
    "            print(i)\n",
    "        \n",
    "            if x in dict.keys():\n",
    "                dict[x] += 1\n",
    "            else:\n",
    "                dict[x] = 1\n",
    "\n",
    "        max_key = max(dict, key=dict.get)\n",
    "        print(max_key)\n",
    "\n",
    "moda(textos)\n",
    "\"\"\"\n",
    "#textos['Moda'] =\n",
    "#Max tiene el máximo tamaño de la palabra\n",
    "textos['Max'] = [[max([len(x) for x in i.split(' ')])][0] for i in textos['Review']]\n",
    "#Min tiene el minimo tamaño de la palabra\n",
    "textos['Min'] = [[min([len(x) for x in i.split(' ')])][0] for i in textos['Review']]\n",
    "\n",
    "def frecuenciaPalabras(texto):\n",
    "    frecuenciaPalabras = {}\n",
    "\n",
    "    for i in texto: \n",
    "        for x in i.split(' '): \n",
    "            if x in frecuenciaPalabras.keys():\n",
    "                frecuenciaPalabras[x] += 1\n",
    "            else:\n",
    "                frecuenciaPalabras[x] = 1\n",
    "\n",
    "    print(frecuenciaPalabras)\n",
    "    return frecuenciaPalabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictFrec = frecuenciaPalabras(textos['Review'])\n",
    "\n",
    "df_Frecuencias = pd.DataFrame.from_dict(dictFrec, orient='index', columns=['frecuencia'])\n",
    "\n",
    "df_Frecuencias['palabra'] = df_Frecuencias.index\n",
    "df_Frecuencias.reset_index(drop=True,inplace=True)\n",
    "\n",
    "\n",
    "df_Frecuencias['palabra']=[unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore') for word in df_Frecuencias['palabra']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedFirst = df_Frecuencias.copy().sort_values(by=['frecuencia'], ascending=False).head(30)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "plt.barh(sortedFirst['palabra'], sortedFirst['frecuencia'], color='green')\n",
    "plt.xlabel('Frecuencia')\n",
    "plt.ylabel('Palabra')\n",
    "plt.title('Distribución de frecuencia de las palabras')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedLast= df_Frecuencias.copy().sort_values(by=['frecuencia'], ascending=False).tail(30)\n",
    "\n",
    "fig2 = plt.figure(figsize=(10, 6))\n",
    "plt.barh(sortedLast['palabra'], sortedLast['frecuencia'], color='green')\n",
    "plt.xlabel('Frecuencia')\n",
    "plt.ylabel('Palabra')\n",
    "plt.title('Distribución de frecuencia de las palabras')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import ydata_profiling\n",
    "from ydata_profiling import ProfileReport\n",
    "ProfileReport(textos)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Limpieza de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4.1 Duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textos.duplicated(keep = False).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textos.drop_duplicates(keep='first', inplace=True)\n",
    "textos.duplicated(keep = False).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(textos['Review'][1201])\n",
    "textos['Review'] = textos['Review'].replace(r'\\d+,\\d+', '', regex=True)\n",
    "textos['Review'] = textos['Review'].replace(r'\\d+', '', regex=True)\n",
    "textos['Review'] = textos['Review'].replace(r'\\d+.\\d+', '', regex=True)\n",
    "print(textos['Review'][1201])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_stopwords = stopwords.words('spanish')\n",
    "print(spanish_stopwords)\n",
    "\n",
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word is not None:\n",
    "          new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "          new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_words.append(word.lower())\n",
    "    return new_words\n",
    "    \n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word is not None:\n",
    "            new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "            if new_word != '':\n",
    "                new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "#def replace_numbers(words):\n",
    "#    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "#    p = inflect.engine()\n",
    "#    print(words)\n",
    "#    new_words = []\n",
    "#    for word in words:\n",
    "#        if word.isdigit():\n",
    "#            new_word = p.number_to_words(word)\n",
    "#            new_words.append(new_word)\n",
    "#            print(\"if \" + new_word)\n",
    "#        else:\n",
    "#            new_words.append(word)\n",
    "#    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in spanish_stopwords:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def preprocessing(words):\n",
    "    words = to_lowercase(words)\n",
    " #   words = replace_numbers(words)\n",
    "    words = remove_punctuation(words)\n",
    "#    words = remove_non_ascii(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4.2 Tokenización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textos['tokens'] = [ WordPunctTokenizer().tokenize(i) for i in textos['Review']]\n",
    "    \n",
    "textos.iloc[1201]['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textos['tokens'].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4.3 Eliminación de ruido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textos['tokens']=textos['tokens'].apply(preprocessing) #Aplica la eliminación del ruido\n",
    "\n",
    "textos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textos['tokens'] = textos['tokens'].apply(lambda x: ' '.join(map(str, x)))\n",
    "textos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4.4 Normalización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "textos['tokens'] =  [ [stemmer.stem(word) for word in tokens] for tokens in textos['tokens']]\n",
    "i = 1201\n",
    "print(textos['tokens'][i])\n",
    "print(textos['Review'][i])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizer(review):\n",
    "\n",
    "    print(review)\n",
    "    \n",
    "    #print(row.name)\n",
    "    doc  =  nlp(review)\n",
    "    #print (review)\n",
    "    lemma = [[word.lemma for word in sent.words]  for sent in doc.sentences]\n",
    "    finalLemma =[]\n",
    "    for sent in lemma:\n",
    "        for word in sent:  \n",
    "            finalLemma.append(word)\n",
    "    #print(finalLemma)\n",
    "\n",
    "    return finalLemma\n",
    "\n",
    "textos['tokens']= lemmatizer(textos['tokens'].values) #Aplica la lematización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def oneSentence(list):\n",
    "    complete = []\n",
    "    for sent in list:\n",
    "        for word in sent:\n",
    "            complete.append(word)\n",
    "\n",
    "    return complete\n",
    "\n",
    "textos['tokens'] = [oneSentence(i) for i in textos['tokens']]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Codigo para guardar los lemas en csv\n",
    "textos.to_csv('lemaSinStopWords.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Leer archivo previamente guardado de lemas\n",
    "from ast import literal_eval\n",
    "textosLemas = pd.read_csv('lemaSinStopWords.csv', sep=',', encoding = 'utf-8')\n",
    "textosLemas['tokens'] = textosLemas['tokens'].apply(literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textosLemas['tokens']=textosLemas['tokens'].apply(preprocessing) #Aplica la eliminación del ruido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4.5 Selección de campos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textosLemas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textosLemas['tokens'] = textosLemas['tokens'].apply(lambda x: ' '.join(map(str, x)))\n",
    "textosLemas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 División en conjuntos train, test y validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = train_test_split(textosLemas, test_size=0.45, random_state=1) \n",
    "print(x_train.shape, x_test.shape)\n",
    "x_train, x_val = train_test_split(x_train, test_size=0.25, random_state=1)\n",
    "\n",
    "print( x_test.shape,x_train.shape, x_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Segundo profile tras aplicación de preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import ydata_profiling\n",
    "from ydata_profiling import ProfileReport\n",
    "ProfileReport(textosLemas)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictFrecuenciasTokenizado = frecuenciaPalabras(textosLemas['tokens'])\n",
    "\n",
    "df_Frecuencias = pd.DataFrame.from_dict(dictFrecuenciasTokenizado, orient='index', columns=['frecuencia'])\n",
    "\n",
    "df_Frecuencias['palabra'] = df_Frecuencias.index\n",
    "df_Frecuencias.reset_index(drop=True,inplace=True)\n",
    "\n",
    "\n",
    "df_Frecuencias['palabra']=[unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore') for word in df_Frecuencias['palabra']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedFirst = df_Frecuencias.copy().sort_values(by=['frecuencia'], ascending=False).head(30)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "plt.barh(sortedFirst['palabra'], sortedFirst['frecuencia'], color='green')\n",
    "plt.xlabel('Frecuencia')\n",
    "plt.ylabel('Palabra')\n",
    "plt.title('Distribución de frecuencia de las palabras')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedLast= df_Frecuencias.copy().sort_values(by=['frecuencia'], ascending=False).tail(30)\n",
    "\n",
    "fig2 = plt.figure(figsize=(10, 6))\n",
    "plt.barh(sortedLast['palabra'], sortedLast['frecuencia'], color='green')\n",
    "plt.xlabel('Frecuencia')\n",
    "plt.ylabel('Palabra')\n",
    "plt.title('Distribución de frecuencia de las palabras')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = x_train['tokens'],x_train['Class']\n",
    "x_val, y_val = x_val['tokens'],x_val['Class']\n",
    "x_test, y_test = x_test['tokens'],x_test['Class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Embedding del texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7.1 Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = CountVectorizer()\n",
    "x_train_countVectorizer = count.fit_transform(x_train)\n",
    "print(x_train_countVectorizer.shape)\n",
    "x_train_countVectorizer.toarray()[3]\n",
    "\n",
    "\n",
    "count = CountVectorizer()\n",
    "x_val_countVectorizer = count.fit_transform(x_val)\n",
    "print(x_val_countVectorizer.shape)\n",
    "x_val_countVectorizer.toarray()[3]\n",
    "\n",
    "\n",
    "count = CountVectorizer()\n",
    "x_test_countVectorizer = count.fit_transform(x_test)\n",
    "print(x_test_countVectorizer.shape)\n",
    "x_test_countVectorizer.toarray()[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7.2 TfiDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "x_train_tfidfVectorizer = tfidf.fit_transform(x_train)\n",
    "print(x_train_tfidfVectorizer.shape)\n",
    "x_train_tfidfVectorizer.toarray()[3]\n",
    "\n",
    "df_train = pd.DataFrame(x_train_tfidfVectorizer[0].T.todense(),\n",
    "    \tindex=tfidf.get_feature_names_out(), columns=[\"TF-IDF\"])\n",
    "df_train = df_train.sort_values('TF-IDF', ascending=False)\n",
    "df_train.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "x_test_tfidfVectorizer = tfidf.fit_transform(x_test)\n",
    "print(x_train_tfidfVectorizer.shape)\n",
    "x_train_tfidfVectorizer.toarray()[3]\n",
    "\n",
    "df_test = pd.DataFrame(x_test_tfidfVectorizer[0].T.todense(),\n",
    "    \tindex=tfidf.get_feature_names_out(), columns=[\"TF-IDF\"])\n",
    "df_test = df_test.sort_values('TF-IDF', ascending=False)\n",
    "df_test.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "x_val_tfidfVectorizer = tfidf.fit_transform(x_val)\n",
    "print(x_train_tfidfVectorizer.shape)\n",
    "x_train_tfidfVectorizer.toarray()[3]\n",
    "\n",
    "df_val = pd.DataFrame(x_val_tfidfVectorizer[0].T.todense(),\n",
    "    \tindex=tfidf.get_feature_names_out(), columns=[\"TF-IDF\"])\n",
    "df_val = df_val.sort_values('TF-IDF', ascending=False)\n",
    "df_val.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Algoritmo KNN (K-Nearest Neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "neigh_cV = neigh.fit(x_train_countVectorizer, y_train)\n",
    "neigh_tfi = neigh.fit(x_train_tfidfVectorizer, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_cV = neigh_cV.predict(x_val_countVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_tfi = neigh_tfi.predict(x_val_tfidfVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se genera la matriz de confusión cV\n",
    "cm_cV = confusion_matrix(y_test, y_pred_cV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se genera la matriz de confusión tfi\n",
    "cm_tfi = confusion_matrix(y_test, y_pred_tfi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se puede visualizar la matriz de confusión cV\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_cV)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se puede visualizar la matriz de confusión tfi\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_tfi)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar reporte de clasificación\n",
    "print(classification_report(y_test, y_pred_cV))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar reporte de clasificación\n",
    "print(classification_report(y_test, y_pred_tfi))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
